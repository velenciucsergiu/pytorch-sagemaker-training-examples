{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on sagemaker with base pytorch image\n",
    "\n",
    "Here all necessary libraries, variables to start the training.\n",
    "This scenario covers training with custom pytorch image, on GPU (1 instance). Training code is in \"code\" directory.\n",
    "!!! NOTE : for VPC training, some additiona settings should be set + we will need a different image, with proxy set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "bucket = \"yourbucker\" \n",
    "kms_key = \"kmskey\"\n",
    "output_path = \"s3://{}/out\".format(bucket)\n",
    "module_path=\"s3://{}/module\".format(bucket)\n",
    "training_data = \"s3//pathtoyourdata\"\n",
    "tensorboard_logs = 's3://{}}/tensorboard/'.format(bucket)\n",
    "\n",
    "named_tuple = time.localtime() # get struct_time\n",
    "base_name = \"pytorch-custom-\"\n",
    "training_job_name = \"{0}{1}\".format(base_name,time.strftime(\"%m-%d-%Y-%H-%M-%S\", named_tuple))\n",
    "checkpoint_s3_uri = \"s3://{0}/checkpoint/{1}\".format(bucket,training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom image will require packing your code yourself\n",
    "\n",
    "Below a pice of code to create an archive with your code and to put it on s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def create_arch(directory, target=None):\n",
    "    if target:\n",
    "        filename = target\n",
    "    else:\n",
    "        _, filename = tempfile.mkstemp()\n",
    "    source_files = os.listdir(os.path.join(os.getcwd(), directory))\n",
    "    source_files = [os.path.join(os.getcwd(),directory, x) for x in source_files]\n",
    "    with tarfile.open(filename, mode=\"w:gz\") as t:\n",
    "        for sf in source_files:\n",
    "            t.add(sf, arcname=os.path.basename(sf))\n",
    "    return filename\n",
    "\n",
    "archive_name = create_arch(\"code\", \"source.tar.gz\")\n",
    "code_path_s3 = module_path+\"/{0}/{1}/{2}\".format(training_job_name,\"code\",archive_name )\n",
    "code_path_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp $archive_name $code_path_s3 --sse aws:kms --sse-kms-key-id $kms_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting sagemaker tensorboard configuration. Will be used during training. Use Tensorflow 2.0 workspace on Sagemaker, to run tensorboard with your tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=tensorboard_logs,\n",
    "    container_local_output_path='/opt/tensorboard/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters. Can be accessed within the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON encode hyperparameters.\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "# s3paths = paths for validation. unfortunately, did not find a good way to have them piped, so, better downloaded locally\n",
    "# train.py is adapted for this, and, will download these files locally, and load during validation\n",
    "hyperparameters = json_encode_hyperparameters({\n",
    "    \"sagemaker_program\": \"train.py\",\n",
    "    \"sagemaker_submit_directory\": module_path_s3,\n",
    "    \"epochs\": 2,\n",
    "    \"learning_rate\" : 2e-5,\n",
    "    \"max_len\" : 128,\n",
    "    \"eps\" : 1e-8,\n",
    "    \"batch_size\" : 20,\n",
    "    \"steps_epoch\" : 1250,\n",
    "    \"s3paths\" : {\n",
    "        \"english\" : \"s3://yourvalidationdata/file.txt\",\n",
    "    }\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What changed from default Pytorch image\n",
    "1. Code is not packing by itself. The above script helps put the code in the right bucket. \n",
    "2. Image name should be specified (dockerfile is in docker directory and should be build and pushed to ECR prior training)\n",
    "3. Entrypoint and module_location should be specified as hyperparameter\n",
    "\n",
    "###### What below code will do:\n",
    "1. Start an estimator object - an abstraction level\n",
    "2. Launch a training job in pipe mode\n",
    "3. In code file, you can specify requirements.txt with python modules to install. Pytorch container will do it for you\n",
    "\n",
    "###### A note on Pipe mode with Pytorch:\n",
    "1. It reads bytes from the file you set as training file. In train.py you specify how many bytes are to be read. \n",
    "2. There is a helper created by me for transforming bytes to dataframe (since we have bytes read from object, it might happen it reads half of the row, or half of the object in the row). It is not perfect (it drops cases he cannot decode), but it helps with the transformation. \n",
    "3. Data will be read sequencially, until the EOF, and will start again on next epoch. Each sequence will be transformed in dataframe (by train.py script) and will be fed for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_uri=\"your image ecr path\",\n",
    "    output_path=output_path,\n",
    "    output_kms_key=kms_key,\n",
    "    role=role,\n",
    "    tensorboard_output_config = tensorboard_output_config,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "    instance_count = 1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_mode='Pipe'\n",
    ")\n",
    "estimator.fit(job_name=training_job_name, inputs ={\"training\":f'{training_data}'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
