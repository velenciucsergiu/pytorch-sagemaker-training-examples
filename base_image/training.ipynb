{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on sagemaker with base pytorch image\n",
    "\n",
    "Here all necessary libraries, variables to start the training.\n",
    "This scenario covers training with default pytorch image, on GPU (1 instance). Training code is in \"code\" directory.\n",
    "!!! NOTE : for VPC training, some additiona settings should be set + we will need a different image, with proxy set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "bucket = \"yourbucker\" \n",
    "kms_key = \"kmskey\"\n",
    "output_path = \"s3://{}/out\".format(bucket)\n",
    "module_path=\"s3://{}/module\".format(bucket)\n",
    "training_data = \"s3//pathtoyourdata\"\n",
    "tensorboard_logs = 's3://{}}/tensorboard/'.format(bucket)\n",
    "\n",
    "named_tuple = time.localtime() # get struct_time\n",
    "base_name = \"pytorch-custom-\"\n",
    "training_job_name = \"{0}{1}\".format(base_name,time.strftime(\"%m-%d-%Y-%H-%M-%S\", named_tuple))\n",
    "checkpoint_s3_uri = \"s3://{0}/checkpoint/{1}\".format(bucket,training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting sagemaker tensorboard configuration. Will be used during training. Use Tensorflow 2.0 workspace on Sagemaker, to run tensorboard with your tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=tensorboard_logs,\n",
    "    container_local_output_path='/opt/tensorboard/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters. Can be accessed within the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON encode hyperparameters.\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "# s3paths = paths for validation. unfortunately, did not find a good way to have them piped, so, better downloaded locally\n",
    "# train.py is adapted for this, and, will download these files locally, and load during validation\n",
    "hyperparameters = json_encode_hyperparameters({\n",
    "    \"epochs\": 2,\n",
    "    \"learning_rate\" : 2e-5,\n",
    "    \"max_len\" : 128,\n",
    "    \"eps\" : 1e-8,\n",
    "    \"batch_size\" : 20,\n",
    "    \"steps_epoch\" : 1250,\n",
    "    \"s3paths\" : {\n",
    "        \"english\" : \"s3://yourvalidationdata/file.txt\",\n",
    "    }\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What below code will do:\n",
    "1. Start an estimator object - an abstraction level class for Pytorch framework\n",
    "2. Pack your code (all content of code directory) and put it to \"module_dir\" s3 path\n",
    "3. Launch a training job in pipe mode\n",
    "4. In code file, you can specify requirements.txt with python modules to install. Pytorch container will do it for you\n",
    "\n",
    "###### A note on Pipe mode with Pytorch:\n",
    "1. It reads bytes from the file you set as training file. In train.py you specify how many bytes are to be read. \n",
    "2. There is a helper created by me for transforming bytes to dataframe (since we have bytes read from object, it might happen it reads half of the row, or half of the object in the row). It is not perfect (it drops cases he cannot decode), but it helps with the transformation. \n",
    "3. Data will be read sequencially, until the EOF, and will start again on next epoch. Each sequence will be transformed in dataframe (by train.py script) and will be fed for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir='code',\n",
    "    entry_point='train.py',\n",
    "    code_location=module_path,\n",
    "    output_path=output_path,\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    output_kms_key=kms_key,\n",
    "    role=role,\n",
    "    tensorboard_output_config = tensorboard_output_config,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "    instance_count = 1,\n",
    "    hyperparameters = hyperparameters,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    input_mode='Pipe'\n",
    ")\n",
    "estimator.fit(job_name=training_job_name, inputs ={\"training\":f'{training_data}'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
